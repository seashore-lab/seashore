# @seashorelab/evaluation

This package provides evaluation tools for assessing agent performance. It supports both rule-based and LLM-based metrics, batch evaluation, and custom metrics.

## Creating an Evaluator

Set up an evaluator with metrics:

```ts
import { createEvaluator, relevanceMetric, coherenceMetric } from '@seashorelab/evaluation';
import { createAgent } from '@seashorelab/agent';

const agent = createAgent({
  name: 'qa-agent',
  model: openaiText('gpt-4o'),
});

const evaluatorAgent = createAgent({
  name: 'evaluator',
  model: openaiText('gpt-4o'),
});

const evaluator = createEvaluator({
  metrics: [
    relevanceMetric({ threshold: 0.7, weight: 1.0 }),
    coherenceMetric({ threshold: 0.6, weight: 0.8 }),
  ],
  llmAdapter: {
    async generate(prompt) {
      const result = await evaluatorAgent.run(prompt);
      return result.content;
    },
  },
  concurrency: 2, // Parallel evaluations
});
```

## Rule-Based Metrics

Create metrics based on rules:

```ts
import { customMetric } from '@seashorelab/evaluation';

const lengthMetric = customMetric({
  name: 'length_check',
  description: 'Check if answer length is appropriate',
  type: 'rule',
  threshold: 0.8,
  evaluate: (input, output) => {
    const len = output.length;
    const passed = len >= 50 && len <= 500;
    return {
      score: passed ? 1.0 : 0.5,
      reason: passed ? 'Length is appropriate' : `Length is ${len}, expected 50-500`,
    };
  },
});

const keywordMetric = customMetric({
  name: 'keyword_presence',
  description: 'Check if required keywords are present',
  type: 'rule',
  threshold: 1.0,
  evaluate: (input, output) => {
    const keywords = ['TypeScript', 'JavaScript'];
    const present = keywords.filter(k => output.includes(k)).length;
    const score = present / keywords.length;
    return {
      score,
      reason: `Found ${present}/${keywords.length} keywords`,
    };
  },
});
```

## LLM-Based Metrics

Use LLM for complex evaluation:

```ts
import { relevanceMetric, faithfulnessMetric, coherenceMetric } from '@seashorelab/evaluation';

const metrics = [
  // Relevance: Does the answer address the question?
  relevanceMetric({
    threshold: 0.7,
    weight: 1.0,
  }),

  // Faithfulness: Is the answer faithful to the context?
  faithfulnessMetric({
    threshold: 0.8,
    weight: 1.0,
  }),

  // Coherence: Is the answer well-structured?
  coherenceMetric({
    threshold: 0.6,
    weight: 0.8,
  }),
];
```

## Single Evaluation

Evaluate a single test case:

```ts
import { evaluate } from '@seashorelab/evaluation';

const result = await evaluate({
  evaluator,
  input: 'What is TypeScript?',
  output: 'TypeScript is a superset of JavaScript.',
  reference: 'TypeScript adds static typing to JavaScript.',
});

console.log('Score:', result.overallScore);
console.log('Passed:', result.passed);
console.log('Details:', result.details);
```

## Batch Evaluation

Evaluate multiple test cases:

```ts
import { evaluateBatch } from '@seashorelab/evaluation';

const testCases = [
  {
    id: 'q1',
    input: 'What is TypeScript?',
    reference: 'TypeScript adds static typing to JavaScript.',
  },
  {
    id: 'q2',
    input: 'What are React hooks?',
    reference: 'Functions that let you use state and lifecycle features.',
  },
];

// First generate outputs
for (const testCase of testCases) {
  const result = await agent.run(testCase.input);
  testCase.output = result.content;
}

// Then evaluate
const batchResult = await evaluateBatch({
  evaluator,
  testCases,
  onProgress: (completed, total) => {
    console.log(`Progress: ${completed}/${total}`);
  },
});

console.log('Pass rate:', batchResult.passRate);
console.log('Average score:', batchResult.overallAverage);
console.log('Duration:', batchResult.durationMs);
```

## Test Cases

Define test cases for evaluation:

```ts
import type { TestCase } from '@seashorelab/evaluation';

const testCases: TestCase[] = [
  {
    id: 'test-1',
    input: 'What is the capital of France?',
    reference: 'Paris',
    // output is generated by agent
  },
  {
    id: 'test-2',
    input: 'Explain React hooks',
    reference: 'React hooks are functions that enable state and lifecycle features in functional components',
    metadata: {
      category: 'react',
      difficulty: 'intermediate',
    },
  },
];
```

## Custom Evaluation Logic

Create custom evaluation functions:

```ts
import { createEvaluator } from '@seashorelab/evaluation';

const customEvaluator = createEvaluator({
  metrics: [myMetric],
  llmAdapter: myAdapter,
  concurrency: 1,
  // Custom evaluation logic
  evaluate: async (testCase, metrics, llmAdapter) => {
    const results = [];

    for (const metric of metrics) {
      if (metric.type === 'rule') {
        const result = await metric.evaluate(testCase.input, testCase.output);
        results.push({
          metric: metric.name,
          score: result.score,
          passed: result.score >= metric.threshold,
          reason: result.reason,
        });
      }
    }

    const overallScore = calculateWeightedScore(results, metrics);
    const passed = results.every(r => r.passed);

    return {
      input: testCase.input,
      output: testCase.output,
      reference: testCase.reference,
      overallScore,
      passed,
      details: results,
      metadata: testCase.metadata,
    };
  },
});
```

## Datasets

Manage test datasets:

```ts
import { createDataset } from '@seashorelab/evaluation';

const dataset = createDataset({
  name: 'qa-benchmark',
  description: 'Question answering benchmark',
  testCases: [
    { id: '1', input: 'Q1', reference: 'A1' },
    { id: '2', input: 'Q2', reference: 'A2' },
  ],
  metadata: {
    version: '1.0',
    created: new Date().toISOString(),
  },
});

// Save dataset
await dataset.save('./datasets/qa-benchmark.json');

// Load dataset
const loaded = await createDataset.from('./datasets/qa-benchmark.json');
```

## Evaluation Result Format

Understanding evaluation results:

```ts
interface EvaluationResult {
  input: string;
  output: string;
  reference?: string;
  overallScore: number; // 0-1
  passed: boolean;
  details: MetricDetail[];
  metadata?: Record<string, unknown>;
}

interface MetricDetail {
  metric: string;
  score: number; // 0-1
  passed: boolean;
  reason?: string;
}

interface BatchResult {
  results: EvaluationResult[];
  passedCount: number;
  failedCount: number;
  passRate: number; // 0-1
  overallAverage: number; // 0-1
  durationMs: number;
}
```

## Comparing Evaluations

Compare multiple evaluation runs:

```ts
import { compareEvaluations } from '@seashorelab/evaluation';

const comparison = compareEvaluations({
  name: 'Baseline vs Experiment',
  baseline: baselineResult,
  experiment: experimentResult,
});

console.log('Improvement:', comparison.improvement);
console.log('Regression:', comparison.regression);
console.log('Significant changes:', comparison.significantChanges);
```

## Progressive Evaluation

Track improvements over time:

```ts
import { createEvaluationHistory } from '@seashorelab/evaluation';

const history = createEvaluationHistory({
  storagePath: './evaluations/history',
});

// Save evaluation
await history.save({
  timestamp: Date.now(),
  version: '1.0.0',
  result: batchResult,
  config: { metrics: [...] },
});

// Get trends
const trends = await history.getTrends('passRate');
console.log('Trend:', trends); // [0.75, 0.80, 0.85, ...]
```

## Async Metrics

Create metrics that require async operations:

```ts
const asyncMetric = customMetric({
  name: 'api_validation',
  description: 'Validate output against external API',
  type: 'rule',
  threshold: 0.9,
  evaluate: async (input, output) => {
    // Call external validation API
    const response = await fetch('https://validation-api.com/check', {
      method: 'POST',
      body: JSON.stringify({ input, output }),
    });
    const result = await response.json();

    return {
      score: result.confidence,
      reason: result.message,
    };
  },
});
```
